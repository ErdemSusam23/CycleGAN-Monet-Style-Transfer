{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNAOTWdf7oaB6pJWrYwWhsY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import zipfile\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "og1h2hmzfXJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=None):\n",
        "    if seed is None:\n",
        "        seed = int(time.time())  # Use current time as seed\n",
        "    print(f\"Using seed: {seed}\")  # Optional: print to know what seed was used\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()  # No need to pass anything; it will randomize"
      ],
      "metadata": {
        "id": "oBQZ9pb1fiCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9121d709-3082-4d60-ed3f-0ca1972eb8c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using seed: 1746545948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3kFoxCph9uY",
        "outputId": "df9ed9c6-664d-4c7d-84c2-2280e78e1ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and extract datasets\n",
        "def download_and_extract_dataset():\n",
        "    # Download Monet dataset from Kaggle\n",
        "    !pip install -q kaggle\n",
        "\n",
        "    #upload kaggle.json\n",
        "    from google.colab import files\n",
        "    files.upload()\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    # Download the dataset (Kaggle GAN competition dataset with Monet paintings)\n",
        "    !kaggle competitions download -c gan-getting-started\n",
        "\n",
        "    # Extract the dataset\n",
        "    with zipfile.ZipFile('gan-getting-started.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('data')\n",
        "\n",
        "    print(\"Dataset downloaded and extracted.\")"
      ],
      "metadata": {
        "id": "171k2lAjiJAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MonetDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with Monet images\n",
        "            transform (callable, optional): Optional transform to be applied on a sample\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image"
      ],
      "metadata": {
        "id": "wpB-XLg-qedL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PhotoDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with photo images\n",
        "            transform (callable, optional): Optional transform to be applied on a sample\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image"
      ],
      "metadata": {
        "id": "gePC7iZPqjlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedTransform:\n",
        "    def __init__(self, base_size=286, crop_size=256, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1):\n",
        "        \"\"\"\n",
        "        Enhanced data augmentation pipeline\n",
        "\n",
        "        Args:\n",
        "            base_size: Size to resize images to before random cropping\n",
        "            crop_size: Size of random crop\n",
        "            brightness: Brightness jitter factor\n",
        "            contrast: Contrast jitter factor\n",
        "            saturation: Saturation jitter factor\n",
        "            hue: Hue jitter factor\n",
        "        \"\"\"\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(base_size),\n",
        "            transforms.RandomCrop(crop_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(p=0.1),  # Add vertical flips\n",
        "            transforms.RandomRotation(degrees=5),  # Small random rotations\n",
        "            transforms.ColorJitter(brightness=brightness, contrast=contrast,\n",
        "                                  saturation=saturation, hue=hue),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return self.transform(img)"
      ],
      "metadata": {
        "id": "K482-YCISFuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual Block for Generator\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, 3),\n",
        "            nn.InstanceNorm2d(in_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, 3),\n",
        "            nn.InstanceNorm2d(in_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)"
      ],
      "metadata": {
        "id": "hcFni7PfqmFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_channels=3, output_channels=3, n_residual_blocks=9):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Initial convolution block\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(input_channels, 64, 7),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "\n",
        "        # Downsampling\n",
        "        in_features = 64\n",
        "        out_features = in_features * 2\n",
        "        for _ in range(2):\n",
        "            model += [\n",
        "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "            out_features = in_features * 2\n",
        "\n",
        "        # Residual blocks\n",
        "        for _ in range(n_residual_blocks):\n",
        "            model += [ResidualBlock(in_features)]\n",
        "\n",
        "        # Upsampling\n",
        "        out_features = in_features // 2\n",
        "        for _ in range(2):\n",
        "            model += [\n",
        "                nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "            out_features = in_features // 2\n",
        "\n",
        "        # Output layer\n",
        "        model += [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(64, output_channels, 7),\n",
        "            nn.Tanh()\n",
        "        ]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "chXJZ-Vqqol0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator Network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # PatchGAN discriminator - 70x70 receptive field\n",
        "        model = [\n",
        "            nn.Conv2d(input_channels, 64, 4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "\n",
        "        model += [\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "\n",
        "        model += [\n",
        "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "\n",
        "        model += [\n",
        "            nn.Conv2d(256, 512, 4, padding=1),\n",
        "            nn.InstanceNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "\n",
        "        # Output layer\n",
        "        model += [nn.Conv2d(512, 1, 4, padding=1)]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "WHPaQd31qqzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_instance_noise(x, std=0.1):\n",
        "    \"\"\"Add Gaussian noise to help stabilize GAN training\"\"\"\n",
        "    return x + torch.randn_like(x) * std"
      ],
      "metadata": {
        "id": "2uAu_1LWGbNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss functions\n",
        "class CycleGANLoss:\n",
        "    def __init__(self, lambda_A=10.0, lambda_B=10.0, lambda_identity=0.05):\n",
        "        self.criterionGAN = nn.MSELoss()\n",
        "        self.criterionCycle = nn.L1Loss()\n",
        "        self.criterionIdentity = nn.L1Loss()\n",
        "        self.lambda_A = lambda_A\n",
        "        self.lambda_B = lambda_B\n",
        "        self.lambda_identity = lambda_identity\n",
        "\n",
        "    def get_generator_loss(self, real_A, real_B, G_AB, G_BA, D_A, D_B):\n",
        "        # Identity loss\n",
        "        identity_A = G_BA(real_A)\n",
        "        identity_B = G_AB(real_B)\n",
        "        loss_identity_A = self.criterionIdentity(identity_A, real_A) * self.lambda_A * self.lambda_identity\n",
        "        loss_identity_B = self.criterionIdentity(identity_B, real_B) * self.lambda_B * self.lambda_identity\n",
        "\n",
        "        # GAN loss\n",
        "        fake_B = G_AB(real_A)\n",
        "        pred_fake_B = D_B(fake_B)\n",
        "        target_real = torch.ones_like(pred_fake_B).to(device)\n",
        "        loss_GAN_AB = self.criterionGAN(pred_fake_B, target_real)\n",
        "\n",
        "        fake_A = G_BA(real_B)\n",
        "        pred_fake_A = D_A(fake_A)\n",
        "        loss_GAN_BA = self.criterionGAN(pred_fake_A, target_real)\n",
        "\n",
        "        # Cycle loss\n",
        "        recovered_A = G_BA(fake_B)\n",
        "        loss_cycle_A = self.criterionCycle(recovered_A, real_A) * self.lambda_A\n",
        "\n",
        "        recovered_B = G_AB(fake_A)\n",
        "        loss_cycle_B = self.criterionCycle(recovered_B, real_B) * self.lambda_B\n",
        "\n",
        "        # Total generator loss\n",
        "        loss_G = loss_identity_A + loss_identity_B + loss_GAN_AB + loss_GAN_BA + loss_cycle_A + loss_cycle_B\n",
        "\n",
        "        return loss_G, {\n",
        "            'loss_G': loss_G.item(),\n",
        "            'loss_identity_A': loss_identity_A.item(),\n",
        "            'loss_identity_B': loss_identity_B.item(),\n",
        "            'loss_GAN_AB': loss_GAN_AB.item(),\n",
        "            'loss_GAN_BA': loss_GAN_BA.item(),\n",
        "            'loss_cycle_A': loss_cycle_A.item(),\n",
        "            'loss_cycle_B': loss_cycle_B.item()\n",
        "        }\n",
        "\n",
        "    def get_discriminator_loss(self, real_A, real_B, fake_A, fake_B, D_A, D_B):\n",
        "\n",
        "        # Add noise to inputs\n",
        "        noise_std = 0.1\n",
        "        real_A = add_instance_noise(real_A, noise_std)\n",
        "        real_B = add_instance_noise(real_B, noise_std)\n",
        "        fake_A = add_instance_noise(fake_A.detach(), noise_std)\n",
        "        fake_B = add_instance_noise(fake_B.detach(), noise_std)\n",
        "\n",
        "        # Real loss\n",
        "        pred_real_A = D_A(real_A)\n",
        "        target_real = torch.ones_like(pred_real_A).to(device)\n",
        "        loss_D_real_A = self.criterionGAN(pred_real_A, target_real)\n",
        "\n",
        "        pred_real_B = D_B(real_B)\n",
        "        loss_D_real_B = self.criterionGAN(pred_real_B, target_real)\n",
        "\n",
        "        # Fake loss\n",
        "        pred_fake_A = D_A(fake_A.detach())\n",
        "        target_fake = torch.zeros_like(pred_fake_A).to(device)\n",
        "        loss_D_fake_A = self.criterionGAN(pred_fake_A, target_fake)\n",
        "\n",
        "        pred_fake_B = D_B(fake_B.detach())\n",
        "        loss_D_fake_B = self.criterionGAN(pred_fake_B, target_fake)\n",
        "\n",
        "        # Total discriminator loss\n",
        "        loss_D_A = (loss_D_real_A + loss_D_fake_A) * 0.5\n",
        "        loss_D_B = (loss_D_real_B + loss_D_fake_B) * 0.5\n",
        "\n",
        "        return loss_D_A, loss_D_B, {\n",
        "            'loss_D_A': loss_D_A.item(),\n",
        "            'loss_D_B': loss_D_B.item(),\n",
        "            'loss_D_real_A': loss_D_real_A.item(),\n",
        "            'loss_D_fake_A': loss_D_fake_A.item(),\n",
        "            'loss_D_real_B': loss_D_real_B.item(),\n",
        "            'loss_D_fake_B': loss_D_fake_B.item()\n",
        "        }"
      ],
      "metadata": {
        "id": "X8Qww16gqs-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LR scheduler\n",
        "class LRScheduler:\n",
        "    def __init__(self, optimizers, num_epochs, start_decay=100, decay_epochs=100):\n",
        "        self.optimizers = optimizers\n",
        "        self.num_epochs = num_epochs\n",
        "        self.start_decay = start_decay\n",
        "        self.decay_epochs = decay_epochs\n",
        "        self.initial_lr = {}\n",
        "\n",
        "        for optimizer in optimizers:\n",
        "            self.initial_lr[optimizer] = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    def step(self, epoch):\n",
        "        for optimizer in self.optimizers:\n",
        "            if epoch >= self.start_decay:\n",
        "                # Linear decay to zero over decay_epochs\n",
        "                lrd = self.initial_lr[optimizer] / self.decay_epochs\n",
        "                new_lr = self.initial_lr[optimizer] - lrd * (epoch - self.start_decay + 1)\n",
        "                new_lr = max(0.0, new_lr)\n",
        "\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = new_lr"
      ],
      "metadata": {
        "id": "SK5kgHfkqvvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cycle_gan(monet_dataloader, photo_dataloader, num_epochs=300, resume_from_epoch=0, checkpoint_dir='checkpoints'):\n",
        "    \"\"\"\n",
        "    Simplified train_cycle_gan function without early stopping\n",
        "\n",
        "    Args:\n",
        "        monet_dataloader: DataLoader for Monet images\n",
        "        photo_dataloader: DataLoader for photo images\n",
        "        num_epochs: Number of epochs to train\n",
        "        resume_from_epoch: Epoch to resume training from\n",
        "        checkpoint_dir: Directory to save checkpoints\n",
        "    \"\"\"\n",
        "    # Create checkpoint directory if it doesn't exist\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize networks\n",
        "    G_AB = Generator().to(device)  # Photo to Monet\n",
        "    G_BA = Generator().to(device)  # Monet to Photo\n",
        "    D_A = Discriminator().to(device)  # Discriminator for Monet domain\n",
        "    D_B = Discriminator().to(device)  # Discriminator for Photo domain\n",
        "\n",
        "    # Initialize optimizers\n",
        "    optimizer_G = optim.Adam(list(G_AB.parameters()) + list(G_BA.parameters()), lr=0.0001, betas=(0.5, 0.999))\n",
        "    optimizer_D_A = optim.Adam(D_A.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "    optimizer_D_B = optim.Adam(D_B.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
        "\n",
        "    # Initialize LR scheduler\n",
        "    scheduler = LRScheduler([optimizer_G, optimizer_D_A, optimizer_D_B], num_epochs)\n",
        "\n",
        "    # Initialize loss calculator\n",
        "    cycle_loss = CycleGANLoss(lambda_A=10.0, lambda_B=10.0, lambda_identity=0.1)\n",
        "\n",
        "    # Load checkpoint if resuming training\n",
        "    start_epoch = 0\n",
        "    if resume_from_epoch > 0:\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{resume_from_epoch}.pth')\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "            G_AB.load_state_dict(checkpoint['G_AB'])\n",
        "            G_BA.load_state_dict(checkpoint['G_BA'])\n",
        "            D_A.load_state_dict(checkpoint['D_A'])\n",
        "            D_B.load_state_dict(checkpoint['D_B'])\n",
        "            optimizer_G.load_state_dict(checkpoint['optimizer_G'])\n",
        "            optimizer_D_A.load_state_dict(checkpoint['optimizer_D_A'])\n",
        "            optimizer_D_B.load_state_dict(checkpoint['optimizer_D_B'])\n",
        "            start_epoch = resume_from_epoch\n",
        "            print(f\"Resuming training from epoch {resume_from_epoch}\")\n",
        "        else:\n",
        "            print(f\"No checkpoint found at {checkpoint_path}, starting from scratch\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
        "        # Update learning rates\n",
        "        scheduler.step(epoch)\n",
        "\n",
        "        # Initialize metrics\n",
        "        metrics = {'loss_G': 0, 'loss_D_A': 0, 'loss_D_B': 0,\n",
        "                  'loss_cycle_A': 0, 'loss_cycle_B': 0,\n",
        "                  'loss_identity_A': 0, 'loss_identity_B': 0}\n",
        "\n",
        "        # Create iterators for data loaders\n",
        "        monet_iter = iter(monet_dataloader)\n",
        "        photo_iter = iter(photo_dataloader)\n",
        "\n",
        "        # Determine number of iterations per epoch\n",
        "        num_iters = min(len(monet_dataloader), len(photo_dataloader))\n",
        "\n",
        "        # Progress bar\n",
        "        pbar = tqdm(range(num_iters), desc=f\"Epoch {epoch+1}/{start_epoch + num_epochs}\")\n",
        "\n",
        "        for i in pbar:\n",
        "            # Get batch of images\n",
        "            try:\n",
        "                real_A = next(monet_iter).to(device)  # Monet paintings\n",
        "            except StopIteration:\n",
        "                monet_iter = iter(monet_dataloader)\n",
        "                real_A = next(monet_iter).to(device)\n",
        "\n",
        "            try:\n",
        "                real_B = next(photo_iter).to(device)  # Photos\n",
        "            except StopIteration:\n",
        "                photo_iter = iter(photo_dataloader)\n",
        "                real_B = next(photo_iter).to(device)\n",
        "\n",
        "            # Generate fake images\n",
        "            fake_B = G_AB(real_A)  # Monet to Photo\n",
        "            fake_A = G_BA(real_B)  # Photo to Monet\n",
        "\n",
        "            # Train generators\n",
        "            optimizer_G.zero_grad()\n",
        "            loss_G, g_metrics = cycle_loss.get_generator_loss(real_A, real_B, G_AB, G_BA, D_A, D_B)\n",
        "            loss_G.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(list(G_AB.parameters()) + list(G_BA.parameters()), max_norm=10)\n",
        "            optimizer_G.step()\n",
        "\n",
        "            # Train discriminators\n",
        "            optimizer_D_A.zero_grad()\n",
        "            optimizer_D_B.zero_grad()\n",
        "            loss_D_A, loss_D_B, d_metrics = cycle_loss.get_discriminator_loss(real_A, real_B, fake_A, fake_B, D_A, D_B)\n",
        "            loss_D_A.backward()\n",
        "            loss_D_B.backward()\n",
        "            optimizer_D_A.step()\n",
        "            optimizer_D_B.step()\n",
        "\n",
        "            # Update metrics\n",
        "            for k, v in g_metrics.items():\n",
        "                if k in metrics:\n",
        "                    metrics[k] += v\n",
        "            for k, v in d_metrics.items():\n",
        "                if k in metrics:\n",
        "                    metrics[k] += v\n",
        "\n",
        "            # Update progress bar\n",
        "            display_metrics = {k: v/num_iters for k, v in metrics.items()}\n",
        "            pbar.set_postfix(display_metrics)\n",
        "\n",
        "        # Save checkpoint and visualize samples every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0 or epoch == start_epoch:\n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'G_AB': G_AB.state_dict(),\n",
        "                'G_BA': G_BA.state_dict(),\n",
        "                'D_A': D_A.state_dict(),\n",
        "                'D_B': D_B.state_dict(),\n",
        "                'optimizer_G': optimizer_G.state_dict(),\n",
        "                'optimizer_D_A': optimizer_D_A.state_dict(),\n",
        "                'optimizer_D_B': optimizer_D_B.state_dict(),\n",
        "                'metrics': display_metrics\n",
        "            }\n",
        "\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "            # Save individual models every 10 epochs\n",
        "            torch.save(G_AB.state_dict(), os.path.join(checkpoint_dir, f'G_AB_epoch_{epoch+1}.pth'))\n",
        "            torch.save(G_BA.state_dict(), os.path.join(checkpoint_dir, f'G_BA_epoch_{epoch+1}.pth'))\n",
        "\n",
        "            print(f\"Saved checkpoint at epoch {epoch+1}\")\n",
        "\n",
        "            # Visualize samples every 10 epochs\n",
        "            visualize_samples(G_AB, G_BA, monet_dataloader, photo_dataloader, epoch)\n",
        "\n",
        "    print(f\"Training finished after {num_epochs} epochs.\")\n",
        "    return G_AB, G_BA, D_A, D_B"
      ],
      "metadata": {
        "id": "aHxYGFJ_qxoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize sample transformations\n",
        "def visualize_samples(G_AB, G_BA, monet_dataloader, photo_dataloader, epoch):\n",
        "    G_AB.eval()\n",
        "    G_BA.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get sample images - adapt to work with any batch size\n",
        "        # Collect up to 4 images from each domain\n",
        "        monet_samples = []\n",
        "        photo_samples = []\n",
        "\n",
        "        # Get Monet samples\n",
        "        for batch in monet_dataloader:\n",
        "            monet_samples.append(batch)\n",
        "            if len(monet_samples) >= 4:\n",
        "                break\n",
        "\n",
        "        # Get Photo samples\n",
        "        for batch in photo_dataloader:\n",
        "            photo_samples.append(batch)\n",
        "            if len(photo_samples) >= 4:\n",
        "                break\n",
        "\n",
        "        # Concatenate batches and take up to 4 images\n",
        "        real_A = torch.cat(monet_samples, dim=0)[:4].to(device)\n",
        "        real_B = torch.cat(photo_samples, dim=0)[:4].to(device)\n",
        "\n",
        "        # Adjust the number of images to visualize based on what's available\n",
        "        num_samples = min(len(real_A), len(real_B))\n",
        "\n",
        "        # Generate fake images\n",
        "        fake_B = G_AB(real_A)  # Monet to Photo\n",
        "        fake_A = G_BA(real_B)  # Photo to Monet\n",
        "\n",
        "        # Generate recovered images\n",
        "        rec_A = G_BA(fake_B)  # Monet -> Photo -> Monet\n",
        "        rec_B = G_AB(fake_A)  # Photo -> Monet -> Photo\n",
        "\n",
        "        # Create figure - adjust based on available samples\n",
        "        fig, axs = plt.subplots(num_samples, 5, figsize=(15, 3*num_samples))\n",
        "\n",
        "        # Handle case when there's only one row\n",
        "        if num_samples == 1:\n",
        "            axs = axs.reshape(1, -1)\n",
        "\n",
        "        # Helper function to denormalize and show images\n",
        "        def show_tensor_image(tensor, ax, title):\n",
        "            image = tensor.detach().cpu().numpy().transpose(1, 2, 0)\n",
        "            # Denormalize: [-1, 1] -> [0, 1]\n",
        "            image = (image + 1) / 2\n",
        "            image = np.clip(image, 0, 1)\n",
        "            ax.imshow(image)\n",
        "            ax.set_title(title)\n",
        "            ax.axis('off')\n",
        "\n",
        "        # Row titles\n",
        "        row_titles = ['Real Monet', 'Monet to Photo', 'Recovered Monet',\n",
        "                     'Real Photo', 'Photo to Monet', 'Recovered Photo']\n",
        "\n",
        "        # Show images\n",
        "        for i in range(num_samples):\n",
        "            show_tensor_image(real_A[i], axs[i, 0], 'Real Monet')\n",
        "            show_tensor_image(fake_B[i], axs[i, 1], 'Monet to Photo')\n",
        "            show_tensor_image(rec_A[i], axs[i, 2], 'Recovered Monet')\n",
        "            show_tensor_image(real_B[i], axs[i, 3], 'Real Photo')\n",
        "            show_tensor_image(fake_A[i], axs[i, 4], 'Photo to Monet')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'samples_epoch_{epoch+1}.png')\n",
        "        plt.close()\n",
        "\n",
        "    G_AB.train()\n",
        "    G_BA.train()"
      ],
      "metadata": {
        "id": "nAIhmz85q0dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(resume_epoch=0):\n",
        "    # Download and extract the dataset\n",
        "    download_and_extract_dataset()\n",
        "\n",
        "    # Create more aggressive data augmentation\n",
        "    transform = EnhancedTransform(\n",
        "        base_size=300,  # Slightly larger for more crop variety\n",
        "        crop_size=256,\n",
        "        brightness=0.3,  # Stronger color jittering\n",
        "        contrast=0.3,\n",
        "        saturation=0.3,\n",
        "        hue=0.1\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    monet_dataset = MonetDataset(root_dir='data/monet_jpg', transform=transform)\n",
        "    photo_dataset = PhotoDataset(root_dir='data/photo_jpg', transform=transform)\n",
        "\n",
        "    # Create dataloaders\n",
        "    monet_dataloader = DataLoader(monet_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
        "    photo_dataloader = DataLoader(photo_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
        "\n",
        "    print(f\"Monet dataset size: {len(monet_dataset)}\")\n",
        "    print(f\"Photo dataset size: {len(photo_dataset)}\")\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = 'checkpoints'\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "   # Train the CycleGAN with more epochs\n",
        "    G_AB, G_BA, D_A, D_B = train_cycle_gan(\n",
        "        monet_dataloader,\n",
        "        photo_dataloader,\n",
        "        num_epochs=300,\n",
        "        resume_from_epoch=resume_epoch,\n",
        "        checkpoint_dir='checkpoints'\n",
        "    )\n",
        "\n",
        "    # Save final models\n",
        "    torch.save(G_AB.state_dict(), os.path.join(checkpoint_dir, 'G_AB_final.pth'))\n",
        "    torch.save(G_BA.state_dict(), os.path.join(checkpoint_dir, 'G_BA_final.pth'))\n",
        "    torch.save(D_A.state_dict(), os.path.join(checkpoint_dir, 'D_A_final.pth'))\n",
        "    torch.save(D_B.state_dict(), os.path.join(checkpoint_dir, 'D_B_final.pth'))\n",
        "\n",
        "    print(\"Training completed and models saved.\")"
      ],
      "metadata": {
        "id": "aA-6zNmbq22x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to apply Monet style to a single image\n",
        "def apply_monet_style(image_path, model_path='checkpoints/G_BA_final.pth'):\n",
        "    \"\"\"\n",
        "    Apply Monet style to a single image\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to the input image\n",
        "        model_path: Path to the saved model (can be a checkpoint or final model)\n",
        "\n",
        "    Returns:\n",
        "        styled_image: The styled image as a numpy array\n",
        "    \"\"\"\n",
        "    # Load the model\n",
        "    model = Generator().to(device)\n",
        "\n",
        "    # Handle different types of saved models\n",
        "    if model_path.endswith('.pth'):\n",
        "        # Check if it's a checkpoint file or just the model state dict\n",
        "        try:\n",
        "            checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "            # If it's a checkpoint dictionary (has 'G_BA' key)\n",
        "            if isinstance(checkpoint, dict) and 'G_BA' in checkpoint:\n",
        "                model.load_state_dict(checkpoint['G_BA'])\n",
        "                print(f\"Loaded model from checkpoint: {model_path}\")\n",
        "            else:\n",
        "                # If it's just the model state dict\n",
        "                model.load_state_dict(checkpoint)\n",
        "                print(f\"Loaded model state dict: {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            return None\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Define transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(256),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    # Load and transform image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Apply style transfer\n",
        "    with torch.no_grad():\n",
        "        styled_image = model(image_tensor)\n",
        "\n",
        "    # Convert tensor to image\n",
        "    styled_image = styled_image.detach().cpu().squeeze(0).numpy()\n",
        "    styled_image = styled_image.transpose(1, 2, 0)\n",
        "    styled_image = (styled_image + 1) / 2  # Denormalize: [-1, 1] -> [0, 1]\n",
        "    styled_image = np.clip(styled_image, 0, 1)\n",
        "\n",
        "    # Display original and styled images\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    ax1.imshow(np.array(image))\n",
        "    ax1.set_title('Original Image')\n",
        "    ax1.axis('off')\n",
        "\n",
        "    ax2.imshow(styled_image)\n",
        "    ax2.set_title('Monet Style')\n",
        "    ax2.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Save the result\n",
        "    output_path = f\"styled_{os.path.basename(image_path)}\"\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(styled_image)\n",
        "    plt.axis('off')\n",
        "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close()\n",
        "    print(f\"Saved styled image to {output_path}\")\n",
        "\n",
        "    return styled_image"
      ],
      "metadata": {
        "id": "oCxPJcmAq7rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    import sys\n",
        "\n",
        "    # Create parser\n",
        "    parser = argparse.ArgumentParser(description=\"Train or resume CycleGAN for Monet style transfer\")\n",
        "    parser.add_argument('--resume', type=int, default=0, help='Resume training from epoch')\n",
        "    parser.add_argument('--style', type=str, default=None, help='Path to image to style transfer')\n",
        "    parser.add_argument('--model', type=str, default='checkpoints/G_BA_final.pth',\n",
        "                       help='Path to model for style transfer')\n",
        "\n",
        "    # Filter out Colab's injected arguments\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    if args.style:\n",
        "        apply_monet_style(args.style, model_path=args.model)\n",
        "    else:\n",
        "        main(resume_epoch=args.resume)"
      ],
      "metadata": {
        "id": "uun1gx7Hq_gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a30E0xj2YSxH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}